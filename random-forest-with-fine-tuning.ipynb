{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":89016,"databundleVersionId":10225329,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n# Load datasets\nroot = \"/kaggle/input/hackathon-uno-2024-iitmz/\"\ntrain = pd.read_csv(root + 'train.csv')\ntest = pd.read_csv(root + 'test.csv')\n\n# Map the target labels directly\ndisposition_mapping = {'CONFIRMED': 0, 'CANDIDATE': 1, 'FALSE POSITIVE': 2}\ntrain['Disposition'] = train['Disposition'].map(disposition_mapping)\n\n# Drop non-features\nX_train = train.drop(columns=['row_id', 'Disposition'])\ny_train = train['Disposition']\ntest_row_ids = test['row_id']\nX_test = test.drop(columns=['row_id'])\n\n# Handle missing values + scale data\nimputer = SimpleImputer(strategy='mean')\nscaler = StandardScaler()\npoly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n\n# Apply transformations\nX_train = scaler.fit_transform(imputer.fit_transform(X_train))\nX_test = scaler.transform(imputer.transform(X_test))\nX_train = poly.fit_transform(X_train)\nX_test = poly.transform(X_test)\n\n# Train/Test split for validation\nX_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n\n# Define the parameter grid for hyperparameter tuning\nparam_grid = {\n    'n_estimators': [100, 200, 300, 400, 500],\n    'max_depth': [10, 20, 30, 40],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'bootstrap': [True, False]\n}\n\n# Instantiate the Random Forest model\nrf = RandomForestClassifier(random_state=42)\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='f1_macro', n_jobs=-1, verbose=2)\ngrid_search.fit(X_train_split, y_train_split)\n\n# Print the best parameters and best score\nprint(\"Best parameters found: \", grid_search.best_params_)\nprint(\"Best cross-validation Macro F1 Score: \", grid_search.best_score_)\n\n# Fit the model with the best parameters\nbest_rf = grid_search.best_estimator_\nbest_rf.fit(X_train_split, y_train_split)\n\n# Predict on validation set\ny_val_pred = best_rf.predict(X_val)\n\n# Evaluate the model performance\nprint(\"Random Forest Performance on Validation Set:\")\nprint(confusion_matrix(y_val, y_val_pred))\nprint(classification_report(y_val, y_val_pred))\nrf_macro_f1 = f1_score(y_val, y_val_pred, average='macro')\nprint(f\"Random Forest Macro F1 Score: {rf_macro_f1:.4f}\")\n\n# Predict on the test data\ntest_predictions = best_rf.predict(X_test)\n\n# Reverse mapping for submission\nreverse_mapping = {0: 'CONFIRMED', 1: 'CANDIDATE', 2: 'FALSE POSITIVE'}\ntest_data = test.copy()\ntest_data['Disposition'] = test_predictions\ntest_data['Disposition'] = test_data['Disposition'].map(reverse_mapping)\n\n# Create and save the submission file\nsubmission = pd.DataFrame({\n    'row_id': test_row_ids,\n    'Disposition': test_data['Disposition']\n})\nsubmission.to_csv('submission.csv', index=False)\nprint(f\"Submission file saved as: submission.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}